version: "3.4"
services:
  elasticsearch-master:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.1
    environment:
      - bootstrap.memory_lock=true
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - 9200:9200
      - 9300:9300
    stdin_open: true
    tty: true
  kibana:
    container_name: kibana
    image: kibana:7.17.1
    environment:
      - bootstrap.memory_lock=true
      - ELASTICSEARCH_HOSTS=http://elasticsearch-master:9200
      - SERVER_HOST=0.0.0.0
      - NODE_OPTIONS=--max-old-space-size=1800
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch-master
    stdin_open: true
    tty: true
  filebeat:
    user: root
    container_name: filebeat
    image: docker.elastic.co/beats/filebeat:7.17.1
    depends_on:
      - elasticsearch-master
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker:/var/lib/docker
      - ./elk/filebeat.yml:/usr/share/filebeat/filebeat.yml
    command: ["--strict.perms=false"]
    ulimits:
      memlock:
        soft: -1
        hard: -1
    stdin_open: true
    tty: true
    deploy:
      mode: global
  metricbeat:
    user: root
    container_name: metricbeat
    image: docker.elastic.co/beats/metricbeat:7.17.1
    depends_on:
      - elasticsearch-master
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker:/var/lib/docker
      - ./elk/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml
    command: ["--strict.perms=false"]
    ulimits:
      memlock:
        soft: -1
        hard: -1
    stdin_open: true
    tty: true
    deploy:
      mode: global
  elastic-populate:
     image: elasticdump/elasticsearch-dump
     entrypoint: >
       /bin/sh -c "
       apt-get update;
       apt-get install -y curl;
       curl -SLO https://boptest-resources.s3.us-west-2.amazonaws.com/elasticsearch-data/kibana_data.json;
       curl -SLO https://boptest-resources.s3.us-west-2.amazonaws.com/elasticsearch-data/kibana_mapping.json;
       elasticdump --input=kibana_mapping.json --output=http://elasticsearch-master:9200/.kibana  --type=mapping;
       elasticdump --input=kibana_data.json --output=http://elasticsearch-master:9200/.kibana  --type=data;
       "
     depends_on:
       - elasticsearch-master
  web: # main haystack application
    build:
      # the purpose of context one level above dockerfile
      # is to utilize shared code, specifically in db
      dockerfile: alfalfa_web/Dockerfile
      context: .
      target: base
      args:
        - NODE_ENV
    image: ${WEB_REGISTRY_URI}:${VERSION_TAG}
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: host
      - "29043:29043"
    environment:
      - NODE_ENV
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - JOB_QUEUE_URL
      - MONGO_URL
      - MONGO_DB_NAME
      - S3_URL
      - S3_URL_EXTERNAL
      - REDIS_HOST
      - S3_BUCKET
      - REGION
    depends_on:
      - redis
      - mongo
      - goaws
      - worker
      - mc
  # Provides a local queue, conforming to aws Queue (SQS) API
  goaws:
    image: pafortin/goaws
    ports:
      - "4100:4100"
  # Local implementation of s3
  minio:
    image: minio/minio
    entrypoint:
      - minio
      - server
      - /data
    ports:
      - "9000:9000"
    environment:
      - MINIO_ACCESS_KEY=${AWS_ACCESS_KEY_ID}
      - MINIO_SECRET_KEY=${AWS_SECRET_ACCESS_KEY}
  mc:
    image: minio/mc
    environment:
      - MINIO_ACCESS_KEY=${AWS_ACCESS_KEY_ID}
      - MINIO_SECRET_KEY=${AWS_SECRET_ACCESS_KEY}
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      /usr/bin/mc config host add myminio http://minio:9000 $${MINIO_ACCESS_KEY} $${MINIO_SECRET_KEY};
      /usr/bin/mc mb myminio/alfalfa;
      /usr/bin/mc policy public myminio/alfalfa;
      "
    depends_on:
      - minio
  worker:
    build:
      # the purpose of context one level above is to install the entire alfalfa package
      # and to copy the wait-for-it.sh and start_worker.sh files in the deploy directory
      dockerfile: alfalfa_worker/Dockerfile
      context: .
      target: base
    image: ${WORKER_REGISTRY_URI}:${VERSION_TAG}
    environment:
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - JOB_QUEUE_URL
      - MONGO_URL
      - MONGO_DB_NAME
      - LOGLEVEL=INFO
      - NODE_ENV
      - S3_URL
      - REDIS_HOST
      - S3_BUCKET
      - REGION
      - INFLUXDB_DB
      - INFLUXDB_HOST
      - INFLUXDB_ADMIN_USER
      - INFLUXDB_ADMIN_PASSWORD
      - HISTORIAN_ENABLE
    depends_on:
      - redis
      - mongo
      - goaws
      - mc
    #volumes:
    # TODO: Add in worker fmu volume
    # TODO: Add in worker openstudio volume
  mongo:
    image: mongo
    ports:
      - "27017:27017"
  redis:
    image: redis
    ports:
      - "6379:6379"
